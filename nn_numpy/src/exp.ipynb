{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"mnist_train.csv\"\n",
    "data = pd.read_csv(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data.iloc[:, 0:1].to_numpy()\n",
    "pixels = data.iloc[:, 1:].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotEncode(labels):\n",
    "    rows = np.shape(labels)[0]\n",
    "    cols = np.shape(np.unique(labels))[0]\n",
    "    base = np.zeros((rows, cols), dtype = int)\n",
    "    \n",
    "    for index, value in enumerate(labels):\n",
    "        base[index][value] = 1\n",
    "        \n",
    "    return base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_vals(a, n):\n",
    "    return a[:n], a[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn = int(np.shape(pixels)[0]*0.7)\n",
    "trn_labels, valid_labels = split_vals(labels, trn)\n",
    "trn_pixels, valid_pixels = split_vals(pixels, trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(a, mean, std):\n",
    "    return (a - mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = trn_pixels.mean()\n",
    "std = trn_pixels.std()\n",
    "norm_trn_pixels = normalise(trn_pixels, mean, std)\n",
    "norm_valid_pixels = normalise(valid_pixels, mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def show(img, title = None):\n",
    "    plt.imshow(img, cmap = \"gray\")\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "def plot(arr):\n",
    "    sample_to_plot = np.reshape(arr, (28, 28))\n",
    "    show(sample_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear():\n",
    "    def __init__(self, n_input, n_output):\n",
    "        np.random.seed(42)\n",
    "        self.weights = np.random.randn(n_input, n_output) * np.sqrt(2/n_input)\n",
    "        self.bias = np.zeros(n_output)\n",
    "        \n",
    "    def setWeights(self, weights = None):\n",
    "        if weights is not None:\n",
    "            self.weights = weights\n",
    "            \n",
    "    def setBias(self, bias = None):\n",
    "        if bias is not None:\n",
    "            self.bias = bias\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.old_x = x\n",
    "        return np.matmul(x, self.weights) + self.bias\n",
    "\n",
    "    # There will be a gradient wrt each output of this layer that comes from layers ahead\n",
    "    # Therefore shape of grad will be the (n_samples, n_outputs) because that many\n",
    "    # elements can come inside from ahead in the layers.\n",
    "    def backward(self, grad):\n",
    "        # This is averaging over all rows. Thus shape is (n_outputs,)\n",
    "        self.grad_bias = np.mean(grad, axis=0)\n",
    "\n",
    "        # x : (n_samples, n_inputs), grad: (n_samples, n_outputs)\n",
    "        # \"None\" adds a unit axis whereexver specified.\n",
    "        # Therefore matrix multiplication becomes (n_samples, n_inputs, 1) and (n_samples, 1, n_outputs)\n",
    "        # This can be translated to: for all \"samples\" do (n_inputs, 1) * (1, n_outputs)\n",
    "        # This gives for all \"samples\", (n_inputs, n_outputs) ie (n_samples, n_inputs, n_outputs)\n",
    "\n",
    "        # (n_inputs, n_outputs) should be expected because, in linear layer, each input node touches\n",
    "        # all the output nodes.\n",
    "        # Taking average at axis 0 will be taking average across all samples since \"n_samples\" is\n",
    "        # the 0th axis\n",
    "        # As a result shape of weights is (n_inputs, n_outputs)\n",
    "        self.grad_weights = (np.matmul(self.old_x[:, :, None], grad[:, None, :])).mean(axis=0)\n",
    "\n",
    "        # (n_samples, n_outputs) * (n_inputs, n_outputs)T will be (n_samples, n_inputs)\n",
    "        # This is expected because the previous layer this layer will pass it to, will have\n",
    "        # n_inputs number of \"outputs\" in its layer, which will form (n_samples, n_outputs) for that layer\n",
    "        return np.dot(grad, self.weights.transpose())\n",
    "\n",
    "    def __repr__(self):\n",
    "        n_input, n_output = np.shape(self.weights)\n",
    "        return f\"Linear ({n_input},{n_output})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU():\n",
    "    def forward(self, x):\n",
    "        self.old_x = x\n",
    "        return np.clip(x, 0, None)\n",
    "\n",
    "    def backward(self, grad):\n",
    "        return np.where(self.old_x > 0, grad, 0)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"ReLU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax():\n",
    "    def forward(self, x):\n",
    "        x = x - np.reshape(np.max(x, axis = 1), (-1, 1))\n",
    "        self.old_y = np.exp(x) / (np.exp(x).sum(axis=1)[:, None])\n",
    "        return self.old_y\n",
    "\n",
    "    def backward(self, grad):\n",
    "        return self.old_y * (grad - (grad * self.old_y).sum(axis=1)[:, None])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Softmax\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy():\n",
    "    def forward(self, x, y):\n",
    "        self.old_x = x.clip(min=1e-8, max=None)\n",
    "        self.old_y = y\n",
    "        return (np.where(y == 1, -np.log(self.old_x), 0)).sum(axis=1)\n",
    "\n",
    "    def backward(self):\n",
    "        return np.where(self.old_y == 1, -1 / self.old_x, 0)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Cross-Entropy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid():\n",
    "    def forward(self, x):\n",
    "        self.old_y = np.exp(x)/(1. + np.exp(x))\n",
    "        return self.old_y\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        differentiation = self.old_y * (1 - self.old_y)\n",
    "        return differentiation * grad\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Sigmoid\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, layers, cost):\n",
    "        self.layers = layers\n",
    "        self.cost = cost\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(f\"Before processing: {np.shape(x)}\")\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        #    print(f\"After passing through {layer} : {np.shape(x)}\")\n",
    "        return x\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def loss(self, x, y):\n",
    "        l = self.cost.forward(self.forward(x), y)\n",
    "        #print(f\"After passing through {self.cost} : {np.shape(l)}\")\n",
    "        return l\n",
    "\n",
    "    def backward(self):\n",
    "        grad = self.cost.backward()\n",
    "        #print(f\"After backward on {self.cost} : {np.shape(grad)}\")\n",
    "\n",
    "        for i in range(len(self.layers) - 1, -1, -1):\n",
    "            grad = self.layers[i].backward(grad)\n",
    "         #   print(f\"After backward on {self.layers[i]} : {np.shape(grad)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [Linear(784, 20), ReLU(), Linear(20, 10), Softmax()]\n",
    "cost = CrossEntropy()\n",
    "model = Model(layers = layers, cost = cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "learning_rate = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, inputs, labels, epochs=1, learning_rate=0.1):\n",
    "    for i in range(epochs):\n",
    "        l = model.loss(inputs, labels).sum()\n",
    "\n",
    "        model.backward()\n",
    "\n",
    "        for layer in model.layers:\n",
    "            if type(layer) is Linear:\n",
    "                layer.weights -= learning_rate * layer.grad_weights\n",
    "                layer.bias -= learning_rate * layer.grad_bias\n",
    "                \n",
    "        print(f\"total loss: {l}, inputs: {np.shape(inputs)[0]}, average loss: {l/np.shape(inputs)[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total loss: 19891.955770908906, inputs: 7000, average loss: 2.841707967272701\n",
      "total loss: 14771.314874160618, inputs: 7000, average loss: 2.1101878391658024\n",
      "total loss: 13384.901270106304, inputs: 7000, average loss: 1.9121287528723292\n",
      "total loss: 12188.518853317266, inputs: 7000, average loss: 1.7412169790453238\n",
      "total loss: 11092.170626703577, inputs: 7000, average loss: 1.5845958038147967\n",
      "total loss: 10090.848818871971, inputs: 7000, average loss: 1.4415498312674244\n",
      "total loss: 9215.956073815483, inputs: 7000, average loss: 1.3165651534022118\n",
      "total loss: 8440.307906310703, inputs: 7000, average loss: 1.2057582723301006\n",
      "total loss: 7782.041848054709, inputs: 7000, average loss: 1.1117202640078154\n",
      "total loss: 7237.650212252598, inputs: 7000, average loss: 1.0339500303217997\n",
      "total loss: 6841.737538930327, inputs: 7000, average loss: 0.9773910769900467\n",
      "total loss: 6669.641083163524, inputs: 7000, average loss: 0.9528058690233606\n",
      "total loss: 6913.719919276392, inputs: 7000, average loss: 0.9876742741823417\n",
      "total loss: 7465.25574049502, inputs: 7000, average loss: 1.066465105785003\n",
      "total loss: 7386.8885221654455, inputs: 7000, average loss: 1.0552697888807778\n",
      "total loss: 6416.2564834455625, inputs: 7000, average loss: 0.9166080690636518\n",
      "total loss: 5661.546850483228, inputs: 7000, average loss: 0.8087924072118897\n",
      "total loss: 5090.909657677799, inputs: 7000, average loss: 0.7272728082396857\n",
      "total loss: 4767.853565481737, inputs: 7000, average loss: 0.6811219379259624\n",
      "total loss: 4552.464053455951, inputs: 7000, average loss: 0.6503520076365644\n"
     ]
    }
   ],
   "source": [
    "train(model = model, inputs = norm_trn_pixels, labels = oneHotEncode(trn_labels), epochs = epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(predictions, labels):\n",
    "    p = predictions.argmax(axis = 1)\n",
    "    l = labels.reshape(-1,)\n",
    "    return (p == l).sum()/np.shape(p)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model.predict(norm_valid_pixels)\n",
    "acc = compute_accuracy(p, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.795"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model.predict(norm_trn_pixels)\n",
    "acc = compute_accuracy(p, trn_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8162857142857143"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dumpParamsInJson(model):\n",
    "    params = dict()\n",
    "    filename = \"nn_params.json\"\n",
    "\n",
    "    for layer in model.layers:\n",
    "        if type(layer) is Linear:\n",
    "            params[f\"{layer}_weights\"] = layer.weights.tolist()\n",
    "            params[f\"{layer}_bias\"] = layer.bias.tolist()\n",
    "            \n",
    "    with open(filename, \"w\") as file:\n",
    "        json.dump(params, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumpParamsInJson(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadParamsIntoLayers(filename, layers):\n",
    "\n",
    "    with open(filename, \"r\") as file:\n",
    "        params = json.load(file)\n",
    "    \n",
    "    for layer in layers:\n",
    "        if type(layer) is Linear:\n",
    "            key_weight = f\"{layer}_weights\"\n",
    "            key_bias = f\"{layer}_bias\"\n",
    "            layer.setWeights(np.array(params[key_weight]))\n",
    "            layer.setBias(np.array(params[key_bias]))\n",
    "    \n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"nn_params.json\"\n",
    "layersR = [Linear(784, 20), ReLU(), Linear(20, 10), Softmax()]\n",
    "layersR = loadParamsIntoLayers(filename, layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelR = Model(layers = layersR, cost = cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = modelR.predict(norm_trn_pixels[:2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
